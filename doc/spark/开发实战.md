##2020-07-10

### RDD

​	[RDD了解](https://beliefer.blog.csdn.net/article/details/90203726)

```scala
    // 获取第一个partition的RDD
    val firstPartitionRDD = rdd2.mapPartitionsWithIndex { (idx, iter) =>
      if (idx == 0) {
        iter
      } else {
        Iterator.empty
      }
    }
```



### 自定义partitions

> coalesce  该函数用于将RDD进行重分区，使用HashPartitioner。
>
> repartition  该函数其实就是coalesce函数第二个参数为true的实现

```

```

### 查询的量太大如何做

```
SparkSession sparkSession = SparkUtils.getSession();
String sql = "select *  from original.t_ods_cmcc_block_sm where day='20200521'";
Dataset<Row> originalData = sparkSession.sql(sql);
JavaRDD<Row> rowJavaRDD = originalData.javaRDD();
//BufferedWriter writer = new BufferedWriter(new FileWriter("E:\\wordcount\\cmcc\\4.txt"));
rowJavaRDD.foreach(x->{
logger.info(x);
});
```

#### 打包

[打成一个jar](https://www.jianshu.com/p/2f5a52721e1c)

```shell
## 修改源码
val session = SparkSession.builder().getOrCreate()
## 打包编译命令
mvn assembly:assembly
```

#### spark到spark

[spark streaming 输出数据到kafka](https://blog.csdn.net/xueba207/article/details/51275219?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase)

[spark写数据到kafka](https://blog.csdn.net/qq_33872191/article/details/84977762?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase)

### RDD广播形式推送到kafka

```
ClassTag<DataProducer2> tag = scala.reflect.ClassTag$.MODULE$.apply(DataProducer2.class);
Broadcast<DataProducer2> broadcast = sparkSession.sparkContext().broadcast(DataProducer2.getInstance(), tag);
broadcast.getValue().sendData("cmccsmsqueue", json);
```

